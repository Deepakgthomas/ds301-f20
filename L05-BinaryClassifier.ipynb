{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DS 301: Applied Data Modeling and Predictive Analysis**\n",
    "\n",
    "**Lecture 5 – Binary Classification and Performance Measures**\n",
    "\n",
    "# Binary Classifier with MNIST Dataset\n",
    "\n",
    "Nok Wongpiromsarn, 31 August 2020\n",
    "\n",
    "**Credit:** The large portion of the code has been taken from Chapter 3 of Aurélien Géron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get, Visualize, and Prepare the Data for Machine Learning\n",
    "\n",
    "**Load the mnist handwritten digit dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.keys())\n",
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_plots import DigitPlotter\n",
    "\n",
    "num_plots = 5\n",
    "plotter = DigitPlotter((28, 28))\n",
    "plotter.is_binary_cm = True\n",
    "plotter.plot_multiple(x[0:num_plots], y[0:num_plots])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the target (y) from string to unsigned integer (0 to 255)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(type(y[0]))\n",
    "y = y.astype(np.uint8)\n",
    "print(type(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "**Use linear classifiers with stochastic gradient descent (SGD) training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the binary target (5 or not 5)\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# some hyperparameters will have a different defaut value in future versions of scikit-Learn, \n",
    "# such as max_iter and tol. \n",
    "# To be future-proof, we explicitly specify these hyperparameters.\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(x_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures\n",
    "\n",
    "**1. Training Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_train_pred = sgd_clf.predict(x_train)\n",
    "print(metrics.accuracy_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Cross-Validation Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, x_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions made on each test fold\n",
    "# With cross_val_predict, the data is split according to the cv parameter.\n",
    "# Each sample belongs to exactly one test set, and its prediction is computed \n",
    "# with an estimator fitted on the corresponding training set.\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3)\n",
    "\n",
    "# Compute the confusion matrix by passing the target (y_train_5)\n",
    "# and the prediction (y_train_pred)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Precision, Recall and F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_train_5, y_train_pred)\n",
    "recall = recall_score(y_train_5, y_train_pred)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall:    \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_train_5, y_train_pred)\n",
    "print(\"f1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision/Recall Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use precision_recall_curve to compute precision and recall \n",
    "# for all possible thresholds\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "y_scores = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3, \n",
    "                             method=\"decision_function\")\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision and recall versus the decision threshold\n",
    "# The length of precisions and the length of recalls is 1 more than that of thresholds.\n",
    "# The last element of precisions is always 1 and the last element of recalls is always 0.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision versus recall\n",
    "plt.plot(recalls, precisions)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a lowest threshold that gives at least 90% precision\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)\n",
    "precision = precision_score(y_train_5, y_train_pred_90)\n",
    "recall = recall_score(y_train_5, y_train_pred_90)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall:    \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. The ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tpr versus fpr\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"SGDClassifier\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Purely random classifier\") # Dashed diagonal\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate (1-Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the area under the curve (AUC)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
